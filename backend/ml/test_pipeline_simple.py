#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Prueba directa de MODEL-003 - Pipeline de Entrenamiento - GuruInversor

Script de validaci√≥n simplificado para probar el pipeline de entrenamiento.
"""

import sys
import os
import numpy as np
import tensorflow as tf
from pathlib import Path
import tempfile
import shutil

# Suprimir warnings innecesarios
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
tf.config.run_functions_eagerly(True)

def test_pipeline_imports():
    """Probar importaci√≥n del pipeline."""
    print("Probando importaci√≥n de pipeline...")
    
    # Importar el m√≥dulo espec√≠fico
    sys.path.insert(0, str(Path(__file__).parent))
    from training_pipeline import (
        TrainingPipeline, PipelineConfig, TrainingJob, PipelineResults,
        create_training_pipeline
    )
    from trainer import LSTMTrainer, TrainingConfig
    from model_architecture import LSTMConfig
    from preprocessor import ProcessingConfig
    print("OK: Importaciones exitosas")
    assert True

def test_pipeline_config():
    """Probar configuraci√≥n del pipeline."""
    print("\nüîß Probando PipelineConfig...")
    
    from training_pipeline import PipelineConfig
    from trainer import TrainingConfig
    from model_architecture import LSTMConfig
    from preprocessor import ProcessingConfig
    
    # Configuraci√≥n por defecto
    config_default = PipelineConfig()
    assert config_default.max_workers == 4
    assert config_default.update_data == True
    assert config_default.model_types == ['basic', 'advanced']
    assert config_default.min_data_days == 365
    print("‚úÖ Configuraci√≥n por defecto correcta")
    
    # Configuraci√≥n personalizada
    config_custom = PipelineConfig(
        max_workers=2,
        model_types=['basic'],
        min_data_days=100,
        update_data=False
    )
    assert config_custom.max_workers == 2
    assert config_custom.model_types == ['basic']
    assert config_custom.min_data_days == 100
    print("‚úÖ Configuraci√≥n personalizada correcta")
    
    # Verificar configuraciones anidadas
    assert isinstance(config_default.lstm_config, LSTMConfig)
    assert isinstance(config_default.training_config, TrainingConfig)
    assert isinstance(config_default.processing_config, ProcessingConfig)
    print("‚úÖ Configuraciones anidadas correctas")

def test_training_job():
    """Probar trabajos de entrenamiento."""
    print("\nüîß Probando TrainingJob...")
    
    from training_pipeline import TrainingJob
    
    # Trabajo b√°sico
    job = TrainingJob(ticker='AAPL', model_type='basic')
    assert job.ticker == 'AAPL'
    assert job.model_type == 'basic'
    assert job.priority == 1
    assert job.status == 'pending'
    assert job.created_at is not None
    print("‚úÖ Trabajo b√°sico correcto")
    
    # Trabajo con prioridad
    job_priority = TrainingJob(ticker='GOOGL', model_type='advanced', priority=3)
    assert job_priority.priority == 3
    print("‚úÖ Trabajo con prioridad correcto")

def test_pipeline_results():
    """Probar resultados del pipeline."""
    print("\nüîß Probando PipelineResults...")
    
    from training_pipeline import PipelineResults
    from datetime import datetime
    
    # Resultados b√°sicos
    results = PipelineResults()
    assert results.total_jobs == 0
    assert results.completed_jobs == 0
    assert results.failed_jobs == 0
    assert results.success_rate == 0.0
    assert len(results.results) == 0
    assert len(results.errors) == 0
    print("‚úÖ Resultados b√°sicos correctos")
    
    # Resultados con datos
    results.total_jobs = 10
    results.completed_jobs = 8
    results.failed_jobs = 2
    results.start_time = datetime.now()
    results.end_time = datetime.now()
    
    assert results.success_rate == 0.8
    assert results.duration >= 0
    print("‚úÖ M√©tricas calculadas correctamente")

def test_pipeline_initialization():
    """Probar inicializaci√≥n del pipeline."""
    print("\nüîß Probando inicializaci√≥n de TrainingPipeline...")
    
    from training_pipeline import TrainingPipeline, PipelineConfig
    
    # Crear directorio temporal para pruebas
    temp_dir = tempfile.mkdtemp()
    
    config = PipelineConfig(
        max_workers=2,
        model_types=['basic'],
        min_data_days=50,
        update_data=False,
        save_logs=False,
        results_dir=temp_dir
    )
    
    # Crear pipeline
    pipeline = TrainingPipeline(config)
    assert pipeline.config.max_workers == 2
    assert len(pipeline.jobs_queue) == 0
    assert len(pipeline.running_jobs) == 0
    print("‚úÖ Pipeline creado correctamente")
    
    # Verificar directorios
    results_dir = Path(temp_dir)
    assert results_dir.exists()
    assert (results_dir / 'models').exists()
    assert (results_dir / 'reports').exists()
    assert (results_dir / 'logs').exists()
    print("‚úÖ Directorios creados correctamente")
    
    # Limpiar
    shutil.rmtree(temp_dir, ignore_errors=True)
    
    return pipeline

def test_job_management():
    """Probar gesti√≥n de trabajos."""
    print("\nüîß Probando gesti√≥n de trabajos...")
    
    # Crear pipeline para las pruebas
    pipeline = test_pipeline_initialization()
    
    # A√±adir trabajo individual
    job_id = pipeline.add_training_job('AAPL', 'basic', priority=1)
    assert len(pipeline.jobs_queue) == 1
    assert 'AAPL_basic' in job_id
    print("‚úÖ Trabajo individual a√±adido")
    
    # A√±adir m√∫ltiples trabajos
    tickers = ['GOOGL', 'MSFT']
    job_ids = pipeline.add_multiple_jobs(tickers, ['basic'])
    assert len(pipeline.jobs_queue) == 3  # 1 anterior + 2 nuevos
    assert len(job_ids) == 2
    print("‚úÖ M√∫ltiples trabajos a√±adidos")
    
    # Verificar estado
    status = pipeline.get_status()
    assert status['jobs_in_queue'] == 3
    assert status['running_jobs'] == 0
    assert status['completed_jobs'] == 0
    print("‚úÖ Estado del pipeline correcto")

def test_data_validation():
    """Probar validaci√≥n de datos."""
    print("\nüîß Probando validaci√≥n de datos...")
    
    # Crear pipeline para las pruebas
    pipeline = test_pipeline_initialization()
    
    # Validar ticker ficticio (deber√≠a fallar)
    is_valid, message = pipeline.validate_ticker_data('INVALID_TICKER')
    assert not is_valid
    assert 'no encontrado' in message.lower()
    print("‚úÖ Validaci√≥n de ticker inv√°lido correcta")
    
    # Nota: Para ticker v√°lido necesitar√≠amos datos en la base de datos
    # En entorno de prueba, esto es esperado que falle
    print("‚úÖ Validaci√≥n de datos funcional")

def test_model_comparison():
    """Probar comparaci√≥n de modelos."""
    print("\nüîß Probando comparaci√≥n de modelos...")
    
    # Crear pipeline para las pruebas
    pipeline = test_pipeline_initialization()
    
    # Comparar modelos (puede estar vac√≠o en entorno de prueba)
    df = pipeline.compare_models()
    
    # Verificar que retorna DataFrame
    import pandas as pd
    assert isinstance(df, pd.DataFrame)
    print("‚úÖ Comparaci√≥n de modelos retorna DataFrame")
    
    # Comparar modelos por ticker espec√≠fico
    df_ticker = pipeline.compare_models('AAPL')
    assert isinstance(df_ticker, pd.DataFrame)
    print("‚úÖ Comparaci√≥n por ticker funcional")

def test_report_generation():
    """Probar generaci√≥n de reportes."""
    print("\nüîß Probando generaci√≥n de reportes...")
    
    # Crear pipeline para las pruebas
    pipeline = test_pipeline_initialization()
    
    from training_pipeline import PipelineResults
    from datetime import datetime
    
    # Crear resultados de prueba
    results = PipelineResults(
        total_jobs=2,
        completed_jobs=1,
        failed_jobs=1,
        start_time=datetime.now(),
        end_time=datetime.now()
    )
    
    # A√±adir resultado exitoso de prueba
    results.results.append({
        'model_metadata': {'ticker': 'AAPL'},
        'model_name': 'test_model',
        'evaluation': {'rmse': 0.05, 'mae': 0.03},
        'data_stats': {'train_samples': 100, 'val_samples': 20, 'test_samples': 10},
        'training_time': 30.5
    })
    
    # A√±adir error de prueba
    results.errors.append({
        'job': {'ticker': 'GOOGL', 'model_type': 'basic'},
        'error': 'Error de prueba',
        'duration': 5.0
    })
    
    # Generar reporte
    report_path = pipeline.generate_training_report(results)
    
    # Verificar que el archivo se cre√≥
    assert Path(report_path).exists()
    print("‚úÖ Reporte generado correctamente")
    
    # Verificar contenido b√°sico
    with open(report_path, 'r', encoding='utf-8') as f:
        content = f.read()
        assert 'Reporte de Entrenamiento' in content
        assert 'Total de trabajos: 2' in content
        assert 'Completados exitosamente: 1' in content
        assert 'AAPL' in content
    print("‚úÖ Contenido del reporte correcto")

def test_convenience_function():
    """Probar funci√≥n de conveniencia."""
    print("\nüîß Probando funci√≥n de conveniencia...")
    
    from training_pipeline import create_training_pipeline, PipelineConfig
    
    # Verificar que la funci√≥n existe y es callable
    assert callable(create_training_pipeline)
    print("‚úÖ Funci√≥n de conveniencia existe")
    
    # Configuraci√≥n para prueba r√°pida
    config = PipelineConfig(
        max_workers=1,
        model_types=['basic'],
        min_data_days=10,
        update_data=False,
        validate_before_training=False,
        save_logs=False,
        results_dir=tempfile.mkdtemp()
    )
    
    print("‚úÖ Configuraci√≥n para funci√≥n de conveniencia preparada")

def main():
    """Funci√≥n principal de pruebas."""
    print("PRUEBAS MODEL-003 - Pipeline de Entrenamiento - GuruInversor")
    print("=" * 60)
    
    results = []
    
    # 1. Pruebas de importaci√≥n
    try:
        test_pipeline_imports()
        results.append(("Importaciones", True))
    except Exception as e:
        print(f"‚ùå Error en importaciones: {e}")
        results.append(("Importaciones", False))
        return results
    
    # 2. Pruebas de configuraci√≥n
    try:
        test_pipeline_config()
        results.append(("Configuraci√≥n pipeline", True))
    except Exception as e:
        print(f"‚ùå Error en configuraci√≥n: {e}")
        results.append(("Configuraci√≥n pipeline", False))
        return results
    
    # 3. Pruebas de TrainingJob
    try:
        test_training_job()
        results.append(("TrainingJob", True))
    except Exception as e:
        print(f"‚ùå Error en TrainingJob: {e}")
        results.append(("TrainingJob", False))
        return results
    
    # 4. Pruebas de PipelineResults
    try:
        test_pipeline_results()
        results.append(("PipelineResults", True))
    except Exception as e:
        print(f"‚ùå Error en PipelineResults: {e}")
        results.append(("PipelineResults", False))
        return results
    
    # 5. Pruebas de inicializaci√≥n
    try:
        test_pipeline_initialization()
        results.append(("Inicializaci√≥n pipeline", True))
    except Exception as e:
        print(f"‚ùå Error en inicializaci√≥n: {e}")
        results.append(("Inicializaci√≥n pipeline", False))
        return results
    
    # 6. Pruebas de gesti√≥n de trabajos
    try:
        test_job_management()
        results.append(("Gesti√≥n trabajos", True))
    except Exception as e:
        print(f"‚ùå Error en gesti√≥n de trabajos: {e}")
        results.append(("Gesti√≥n trabajos", False))
        return results
    
    # 7. Pruebas de validaci√≥n de datos
    try:
        test_data_validation()
        results.append(("Validaci√≥n datos", True))
    except Exception as e:
        print(f"‚ùå Error en validaci√≥n de datos: {e}")
        results.append(("Validaci√≥n datos", False))
        return results
    
    # 8. Pruebas de comparaci√≥n de modelos
    try:
        test_model_comparison()
        results.append(("Comparaci√≥n modelos", True))
    except Exception as e:
        print(f"‚ùå Error en comparaci√≥n de modelos: {e}")
        results.append(("Comparaci√≥n modelos", False))
        return results
    
    # 9. Pruebas de generaci√≥n de reportes
    try:
        test_report_generation()
        results.append(("Generaci√≥n reportes", True))
    except Exception as e:
        print(f"‚ùå Error en generaci√≥n de reportes: {e}")
        results.append(("Generaci√≥n reportes", False))
        return results
    
    # 10. Pruebas de funci√≥n de conveniencia
    try:
        test_convenience_function()
        results.append(("Funci√≥n conveniencia", True))
    except Exception as e:
        print(f"‚ùå Error en funci√≥n de conveniencia: {e}")
        results.append(("Funci√≥n conveniencia", False))
    
    # Resumen final
    print("\n" + "=" * 60)
    print("üìã RESUMEN DE PRUEBAS MODEL-003")
    print("=" * 60)
    
    passed = sum(1 for _, success in results if success)
    total = len(results)
    
    for test_name, success in results:
        status = "‚úÖ PASSED" if success else "‚ùå FAILED"
        print(f"{test_name:<25} {status}")
    
    print("-" * 60)
    print(f"Total: {passed}/{total} pruebas exitosas ({passed/total*100:.1f}%)")
    
    if passed == total:
        print("\nüéâ ¬°TODAS LAS PRUEBAS DE MODEL-003 PASARON!")
        print("‚úÖ Pipeline de entrenamiento implementado y validado exitosamente")
        print("üöÄ MODEL-003 completado - Listo para MODEL-004")
    else:
        print(f"\n‚ö†Ô∏è  {total-passed} pruebas fallaron")
        print("‚ùå MODEL-003 requiere correcciones antes de continuar")
    
    print("=" * 60)
    return results

if __name__ == "__main__":
    main()
