#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Prueba directa de MODEL-002 - Entrenador LSTM - GuruInversor

Script de validaci√≥n simplificado para probar el entrenador LSTM.
"""

import sys
import os
import numpy as np
import tensorflow as tf
from pathlib import Path

# Suprimir warnings innecesarios
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
tf.config.run_functions_eagerly(True)

def test_trainer_imports():
    """Probar importaci√≥n del entrenador."""
    print("üîß Probando importaci√≥n de trainer...")
    
    try:
        # Importar solo el m√≥dulo espec√≠fico
        sys.path.insert(0, str(Path(__file__).parent))
        from trainer import LSTMTrainer, TrainingConfig, train_lstm_model
        from model_architecture import LSTMConfig
        from preprocessor import ProcessingConfig
        print("‚úÖ Importaciones exitosas")
        return True, LSTMTrainer, TrainingConfig, LSTMConfig
    except Exception as e:
        print(f"‚ùå Error en importaci√≥n: {e}")
        return False, None, None, None

def test_training_config():
    """Probar configuraci√≥n de entrenamiento."""
    print("\nüîß Probando TrainingConfig...")
    
    try:
        from trainer import TrainingConfig
        
        # Configuraci√≥n por defecto
        config_default = TrainingConfig()
        assert config_default.train_split == 0.7
        assert config_default.validation_split == 0.2
        assert config_default.test_split == 0.1
        assert config_default.batch_size == 32
        assert config_default.epochs == 100
        print("‚úÖ Configuraci√≥n por defecto correcta")
        
        # Configuraci√≥n personalizada
        config_custom = TrainingConfig(
            train_split=0.8,
            validation_split=0.15,
            test_split=0.05,
            batch_size=64,
            epochs=50
        )
        assert config_custom.train_split == 0.8
        assert config_custom.batch_size == 64
        print("‚úÖ Configuraci√≥n personalizada correcta")
        
        # Validaci√≥n de splits
        try:
            TrainingConfig(train_split=0.6, validation_split=0.2, test_split=0.1)
            assert False, "Deber√≠a fallar con splits incorrectos"
        except ValueError:
            print("‚úÖ Validaci√≥n de splits funciona")
        
        return True, config_default
    except Exception as e:
        print(f"‚ùå Error en configuraci√≥n: {e}")
        return False, None

def test_trainer_initialization(LSTMTrainer, TrainingConfig, LSTMConfig):
    """Probar inicializaci√≥n del entrenador."""
    print("\nüîß Probando inicializaci√≥n de LSTMTrainer...")
    
    try:
        lstm_config = LSTMConfig(
            sequence_length=20,
            n_features=6,
            lstm_units=[15, 10],
            dense_units=[8, 4]
        )
        
        training_config = TrainingConfig(
            batch_size=16,
            epochs=2,
            min_data_points=100
        )
        
        # Crear entrenador
        trainer = LSTMTrainer(lstm_config, training_config)
        assert trainer is not None
        print("‚úÖ Entrenador creado correctamente")
        
        # Verificar configuraciones
        assert trainer.lstm_config.sequence_length == 20
        assert trainer.training_config.batch_size == 16
        assert trainer.model is None
        assert trainer.history is None
        print("‚úÖ Configuraciones verificadas")
        
        return True, trainer
    except Exception as e:
        print(f"‚ùå Error en inicializaci√≥n: {e}")
        return False, None

def test_model_building(trainer):
    """Probar construcci√≥n de modelos."""
    print("\nüîß Probando construcci√≥n de modelos...")
    
    try:
        # Construir modelo b√°sico
        model = trainer.build_model('basic')
        assert model is not None
        assert model.name == 'LSTM_Basic'
        assert trainer.model is not None
        print("‚úÖ Modelo b√°sico construido")
        
        # Verificar shapes
        expected_input = (None, 20, 6)
        expected_output = (None, 2)
        assert model.input_shape == expected_input
        assert model.output_shape == expected_output
        print("‚úÖ Shapes verificados")
        
        # Construir modelo avanzado
        advanced_model = trainer.build_model('advanced')
        assert advanced_model is not None
        assert advanced_model.name == 'LSTM_Advanced'
        print("‚úÖ Modelo avanzado construido")
        
        # Verificar caracter√≠sticas avanzadas
        layer_names = [layer.name for layer in advanced_model.layers]
        has_bidirectional = any('bidirectional' in name for name in layer_names)
        has_normalization = any('normalization' in name for name in layer_names)
        
        assert has_bidirectional, "Debe tener capas bidireccionales"
        assert has_normalization, "Debe tener normalizaci√≥n"
        print("‚úÖ Caracter√≠sticas avanzadas verificadas")
        
        return True, model
    except Exception as e:
        print(f"‚ùå Error en construcci√≥n de modelo: {e}")
        return False, None

def test_callbacks(trainer):
    """Probar callbacks de entrenamiento."""
    print("\nüîß Probando callbacks de entrenamiento...")
    
    try:
        callbacks = trainer.get_callbacks('test_model')
        
        assert len(callbacks) > 0
        print(f"‚úÖ {len(callbacks)} callbacks configurados")
        
        # Verificar tipos de callbacks
        callback_types = [type(cb).__name__ for cb in callbacks]
        
        required_callbacks = ['EarlyStopping', 'ModelCheckpoint', 'ReduceLROnPlateau', 'TensorBoard']
        for required in required_callbacks:
            assert required in callback_types, f"Falta callback: {required}"
        
        print("‚úÖ Todos los callbacks requeridos presentes")
        return True
    except Exception as e:
        print(f"‚ùå Error en callbacks: {e}")
        return False

def test_model_evaluation(trainer):
    """Probar evaluaci√≥n de modelos."""
    print("\nüîß Probando evaluaci√≥n de modelos...")
    
    try:
        # Datos sint√©ticos para evaluaci√≥n
        X_test = np.random.random((20, 20, 6))
        y_test = np.random.random((20, 2))
          # Evaluar modelo
        metrics = trainer.evaluate_model(X_test, y_test)
        
        assert isinstance(metrics, dict)
        assert 'test_loss' in metrics
        print("‚úÖ Evaluaci√≥n completada")
        
        # Verificar m√©tricas b√°sicas
        assert len(metrics) > 5  # Debe tener varias m√©tricas
          # Verificar m√©tricas
        for metric_name, value in metrics.items():
            # Verificar que sea num√©rico (incluyendo tipos NumPy)
            assert isinstance(value, (int, float, np.integer, np.floating)), f"M√©trica {metric_name} no es num√©rica: {type(value)}"
            if np.isnan(value):
                print(f"‚ö†Ô∏è  M√©trica {metric_name} es NaN (normal en datos sint√©ticos)")
                continue
            if np.isinf(value):
                print(f"‚ö†Ô∏è  M√©trica {metric_name} es infinita (normal en datos sint√©ticos)")
                continue
        
        print(f"‚úÖ {len(metrics)} m√©tricas calculadas correctamente")
        return True
    except Exception as e:
        print(f"‚ùå Error en evaluaci√≥n: {e}")
        return False

def test_predictions(trainer):
    """Probar predicciones del modelo."""
    print("\nüîß Probando predicciones...")
    
    try:
        # Datos sint√©ticos
        X_test = np.random.random((10, 20, 6))
        
        # Hacer predicciones
        predictions = trainer.predict(X_test)
        
        # Verificar resultados
        assert predictions.shape == (10, 2)
        assert not np.isnan(predictions).any()
        assert not np.isinf(predictions).any()
        print("‚úÖ Predicciones exitosas")
        
        print(f"üìä Ejemplo de predicci√≥n: {predictions[0]}")
        return True
    except Exception as e:
        print(f"‚ùå Error en predicci√≥n: {e}")
        return False

def test_training_summary(trainer):
    """Probar resumen de entrenamiento."""
    print("\nüîß Probando resumen de entrenamiento...")
    
    try:
        # Sin entrenamiento
        summary = trainer.get_training_summary()
        assert summary == {}
        print("‚úÖ Resumen vac√≠o correcto")
        
        # Mock de historial
        class MockHistory:
            def __init__(self):
                self.history = {
                    'loss': [0.5, 0.4, 0.3, 0.2],
                    'val_loss': [0.6, 0.5, 0.4, 0.3]
                }
        
        trainer.history = MockHistory()
        trainer.training_start_time = 0
        trainer.training_end_time = 100
        
        summary = trainer.get_training_summary()
        
        required_fields = ['epochs_completed', 'final_loss', 'final_val_loss', 
                          'best_loss', 'best_val_loss', 'training_time']
        
        for field in required_fields:
            assert field in summary, f"Falta campo: {field}"
        
        assert summary['epochs_completed'] == 4
        assert summary['final_loss'] == 0.2
        print("‚úÖ Resumen de entrenamiento correcto")
        
        return True
    except Exception as e:
        print(f"‚ùå Error en resumen: {e}")
        return False

def test_convenience_function():
    """Probar funci√≥n de conveniencia."""
    print("\nüîß Probando funci√≥n de conveniencia...")
    
    try:
        from trainer import train_lstm_model
        from model_architecture import LSTMConfig
        from trainer import TrainingConfig
        
        # Verificar que la funci√≥n es callable
        assert callable(train_lstm_model)
        print("‚úÖ Funci√≥n de conveniencia existe")
        
        # Configuraciones para prueba
        lstm_config = LSTMConfig(
            sequence_length=20,
            n_features=6,
            lstm_units=[10],
            dense_units=[5]
        )
        
        training_config = TrainingConfig(
            epochs=1,
            batch_size=16,
            min_data_points=50
        )
        
        print("‚úÖ Configuraciones para funci√≥n de conveniencia listas")
        return True
    except Exception as e:
        print(f"‚ùå Error en funci√≥n de conveniencia: {e}")
        return False

def main():
    """Funci√≥n principal de pruebas."""
    print("üß™ PRUEBAS MODEL-002 - Entrenador LSTM - GuruInversor")
    print("=" * 60)
    
    results = []
    
    # 1. Pruebas de importaci√≥n
    success, LSTMTrainer, TrainingConfig, LSTMConfig = test_trainer_imports()
    results.append(("Importaciones", success))
    if not success:
        return results
    
    # 2. Pruebas de configuraci√≥n
    success, config = test_training_config()
    results.append(("Configuraci√≥n entrenamiento", success))
    if not success:
        return results
    
    # 3. Pruebas de inicializaci√≥n
    success, trainer = test_trainer_initialization(LSTMTrainer, TrainingConfig, LSTMConfig)
    results.append(("Inicializaci√≥n trainer", success))
    if not success:
        return results
    
    # 4. Pruebas de construcci√≥n de modelo
    success, model = test_model_building(trainer)
    results.append(("Construcci√≥n modelo", success))
    if not success:
        return results
    
    # 5. Pruebas de callbacks
    success = test_callbacks(trainer)
    results.append(("Callbacks", success))
    
    # 6. Pruebas de evaluaci√≥n
    success = test_model_evaluation(trainer)
    results.append(("Evaluaci√≥n", success))
    
    # 7. Pruebas de predicci√≥n
    success = test_predictions(trainer)
    results.append(("Predicci√≥n", success))
    
    # 8. Pruebas de resumen
    success = test_training_summary(trainer)
    results.append(("Resumen entrenamiento", success))
    
    # 9. Funci√≥n de conveniencia
    success = test_convenience_function()
    results.append(("Funci√≥n conveniencia", success))
    
    return results

if __name__ == "__main__":
    # Ejecutar pruebas
    results = main()
    
    # Mostrar resumen
    print("\n" + "=" * 60)
    print("üìã RESUMEN DE PRUEBAS MODEL-002")
    print("=" * 60)
    
    passed = 0
    total = len(results)
    
    for test_name, success in results:
        status = "‚úÖ PASSED" if success else "‚ùå FAILED"
        print(f"{test_name:<25} {status}")
        if success:
            passed += 1
    
    print("-" * 60)
    print(f"Total: {passed}/{total} pruebas exitosas ({passed/total*100:.1f}%)")
    
    if passed == total:
        print("\nüéâ ¬°TODAS LAS PRUEBAS DE MODEL-002 PASARON!")
        print("‚úÖ Entrenador LSTM implementado y validado exitosamente")
        print("üöÄ MODEL-002 completado - Listo para MODEL-003")
    else:
        print(f"\n‚ö†Ô∏è  {total-passed} pruebas fallaron")
        print("‚ùå MODEL-002 requiere correcciones antes de continuar")
    
    print("=" * 60)
